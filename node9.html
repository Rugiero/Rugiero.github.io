<!DOCTYPE HTML>

<!--Converted with LaTeX2HTML 2024 (Released January 1, 2024) -->
<HTML lang="en">
<HEAD>
<TITLE>January&mdash;Machine learning and stochastic geometry are a happy couple</TITLE>

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="viewport" CONTENT="width=device-width, initial-scale=1.0">
<META NAME="Generator" CONTENT="LaTeX2HTML v2024">

<LINK REL="STYLESHEET" HREF="source.css">

<LINK REL="previous" HREF="node8.html">
<LINK REL="next" HREF="node10.html">
</HEAD>

<BODY >

<DIV CLASS="navigation"><!--Navigation Panel-->
<A
 HREF="node10.html">
<IMG WIDTH="37" HEIGHT="24" ALT="next" SRC="next.png"></A> 
<A
 HREF="node8.html">
<IMG WIDTH="26" HEIGHT="24" ALT="up" SRC="up.png"></A> 
<A
 HREF="node8.html">
<IMG WIDTH="63" HEIGHT="24" ALT="previous" SRC="prev.png"></A> 
<A ID="tex2html228"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALT="contents" SRC="contents.png"></A>  <A ID="tex2html2"
  HREF="cv.pdf"><IMG
 STYLE="" SRC="jetscalecropped.png"
 ALT="8"></A>
<BR>
<B> Next:</B> <A
 HREF="node10.html">Blog posts 2023</A>
<B> Up:</B> <A
 HREF="node8.html">Blog posts 2026</A>
<B> Previous:</B> <A
 HREF="node8.html">Blog posts 2026</A>
 &nbsp; <B>  <A ID="tex2html229"
  HREF="node1.html">Contents</A></B> 
<BR>
<BR></DIV>
<!--End of Navigation Panel-->

<H2><A ID="SECTION00031000000000000000">
January&mdash;Machine learning and stochastic geometry are a happy couple</A>
</H2>

<P>
The clear trend is that <A ID="tex2html27"
  HREF="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</A>
(AI) and <A ID="tex2html28"
  HREF="https://en.wikipedia.org/wiki/Machine_learning">machine learning</A>
(ML) are current hot topics, particularly in wireless networks and signal processing. Another active research topic is <A ID="tex2html29"
  HREF="https://en.wikipedia.org/wiki/Stochastic_geometry_models_of_wireless_networks">stochastic geometry</A>
(SG). In this regard, some perceive SG and ML as competitors, suggesting ML has surpassed SG in practical impact. However, this kind of contraption is a poor perspective, and without saying, not the only way to look at it. The truth is that ongoing SG research remains valuable, not only as an academic exercise, but also beyond its academic context. In practice, SG can and will be utilized in modern ML by providing <SPAN  CLASS="textbf">hypothesis classes</SPAN> for the ML algorithms. In this post, I will provide a concrete example of how.

<P>
<BLOCKQUOTE>
<SPAN  CLASS="textit">For instance, SG models can be integrated as a hypothesis class in the learning process of ML.</SPAN>
                           
<DIV class="RIGHT">
&mdash; Yassine H Mamouche et al., [1].
                           
</DIV>
</BLOCKQUOTE>

<P>

<DIV class="CENTER"><A ID="99"></A>
<TABLE>
<CAPTION class="BOTTOM"><STRONG>Figure 2:</STRONG>
Image generated with the help of DALL-E, an AI by OpenAI.</CAPTION>
<TR><TD><IMG
  WIDTH="603" HEIGHT="603" STYLE=""
 SRC="./SGandML.png"
 ALT="Image SGandML"></TD></TR>
</TABLE>
</DIV>

<P>
Let me formulate the working definitions of the ML and SG. We focus on the signal processing of wireless networks, particularly interference modeling.
                 
<UL>
<LI>SG encompasses <SPAN  CLASS="textbf">stochastic models</SPAN> of wireless networks, from which many statistical properties, such as signal power distributions, can be inquired. The expressions are usually presented in mathematical forms; at best, tractable and closed-form formulas are available. 
</LI>
<LI>ML encompasses learning algorithms that ultimately utilize empirical <SPAN  CLASS="textbf">raw data</SPAN>, and they are powerful tools for signal interpolation and prediction. The results are usually numerical.
                 
</LI>
</UL>

<P>
The landscape of the ML algorithms is vast; however, they are often based on <A ID="tex2html30"
  HREF="https://en.wikipedia.org/wiki/Kriging">Gaussian process regression</A>
(GPR). The GPR is a signal prediction method that relies on estimating the signal using a set of samples (or observations) and a <A ID="tex2html31"
  HREF="https://en.wikipedia.org/wiki/Correlation">correlation</A>
function. More specifically, an <A ID="tex2html32"
  HREF="https://en.wikipedia.org/wiki/Autocorrelation">autocovariance</A>
function (also referred to as a kernel) is at the heart of the GPR. Such an autocovariance function can be arbitrary, and one should carefully choose an appropriate one to gain meaningful results. One way is to numerically estimate the autocovariance from <A ID="tex2html33"
  HREF="https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets">training data</A>. Whether the training data is empirical data from an actual system or generated synthetically by using simulation tools, the numerical statistical estimates lack generality and physical insight, and have a concomitant dependence on local, empirical data and circumstances. Where to even start interpreting such data, if we don't have a rigid understanding of what qualities we are looking for? It is time for the SG to step into the arena: A comprehensive, tractable, and solid wireless network SG system model provides hypothesis classes that guide us effectively in inferring patterns from empirical ML signal data.

<P>
In the following, we demonstrate a simple example of estimating the power of a non-stationary Gaussian interference signal at a <A ID="tex2html34"
  HREF="https://en.wikipedia.org/wiki/Low_Earth_orbit">low Earth orbit</A>
(LEO) satellite receiver. The prior autocovariance function is derived from a theoretical estimate based on SG. Based on this autocovariance, the power estimation is inferred from empirical data using GPR. Furthermore, the estimation is compared to a <A ID="tex2html35"
  HREF="https://en.wikipedia.org/wiki/Moving_average">moving average</A>
(MA) estimation. 

<P>
The theoretical settings are:
                 
<OL>
<LI>The interferers (natural sources or transmitters) of bandwidth <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.72ex; vertical-align: -0.12ex; " SRC="img6.svg"
 ALT="$1$"></SPAN> kHz are distributed according to the <A ID="tex2html21"
  HREF="https://en.wikipedia.org/wiki/Poisson_point_process">Poisson point process</A>
(PPP) on the Earth surface, causing interference at the LEO satellite. 
</LI>
<LI>The LEO satellite has a narrow beam of <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.92ex; vertical-align: -0.31ex; " SRC="img7.svg"
 ALT="$-3$"></SPAN> dB beamwidth <!-- MATH
 $\varphi_{\text{RX}}=1.6^{\circ} = 0.027925$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.28ex; vertical-align: -0.57ex; " SRC="img8.svg"
 ALT="$\varphi_{\text{RX}}=1.6^{\circ} = 0.027925$"></SPAN> rad steered towards the Earth center.
</LI>
<LI>The LEO satellite is at <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.84ex; vertical-align: -0.12ex; " SRC="img9.svg"
 ALT="$h=200$"></SPAN> km moving at its orbital speed <!-- MATH
 $v_{\text{sat}}=7.4$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.07ex; vertical-align: -0.46ex; " SRC="img10.svg"
 ALT="$v_{\text{sat}}=7.4$"></SPAN> km/s.
</LI>
<LI>For simplicity, no fast-<A ID="tex2html22"
  HREF="https://en.wikipedia.org/wiki/Fading">fading</A>, shadowing, or other attenuation is considered. For the narrow beam, the Doppler shifts are very close to each other for all transmitters.
                 
</LI>
</OL>
                 As the satellite moves, the magnitude of the aggregate interference varies due to the randomly distributed interferers. The key insight acquired from the SG analysis is as follows. (The details can be found in <A ID="tex2html36"
  HREF="dissertation.pdf">my thesis</A>, Sections 3.3 and 3.4.)

<P>
<DL class="COMPACT">
<DT>5.</DT>
<DD>Under fairly general settings (like assuming a Gaussian waveform for each interferer), the interference waveform at the LEO satellite is approximately
                   <P><!-- MATH
 \begin{displaymath}
I=I(t)=\sqrt{P(t)}X(t),
\end{displaymath}
 -->
</P>
<DIV CLASS="displaymath">
<IMG
 STYLE="height: 3.10ex; vertical-align: -0.71ex; " SRC="img11.svg"
 ALT="$\displaystyle I=I(t)=\sqrt{P(t)}X(t), $">
</DIV><P></P>
where <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img12.svg"
 ALT="$X=X(t)$"></SPAN> is a zero-mean Gaussian waveform with <!-- MATH
 $\mathbb{E}(|X|^2)=1$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.69ex; vertical-align: -0.70ex; " SRC="img13.svg"
 ALT="$\mathbb{E}(\vert X\vert^2)=1$"></SPAN>, which is modulated by the power <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img14.svg"
 ALT="$P=P(t)$"></SPAN>.
                 
</DD>
<DT>6.</DT>
<DD>The normalized interference power (<SPAN  CLASS="textit">i.e.</SPAN>, variance of <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.81ex; vertical-align: -0.12ex; " SRC="img15.svg"
 ALT="$I$"></SPAN>) is approximately <A ID="tex2html23"
  HREF="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distributed</A>
with the mean <!-- MATH
 $\mathbb{E}(P)=1$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img16.svg"
 ALT="$\mathbb{E}(P)=1$"></SPAN> and the variance <!-- MATH
 $\text{var}(P)=1/2$
 -->
<SPAN CLASS="MATH">var<IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img17.svg"
 ALT="$(P)=1/2$"></SPAN> (see <A ID="tex2html24"
  HREF="dissertation.pdf">thesis</A>, Theorems 3.3.6 and 3.3.7).
                 
</DD>
<DT>7.</DT>
<DD>The autocovariance of <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img18.svg"
 ALT="$P(t)$"></SPAN> at lag <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.23ex; vertical-align: -0.12ex; " SRC="img19.svg"
 ALT="$\tau$"></SPAN> has the <A ID="tex2html25"
  HREF="https://en.wikipedia.org/wiki/Radial_basis_function_kernel">Gaussian form</A>
<P><!-- MATH
 \begin{displaymath}
K_{P}(\tau)=k \exp\left\{-D\tau^2 \log(2)\right\}= \frac{p_t^2}{2\log(2)}  \pi \lambda\left(\frac{h \varphi_{\text{RX}}}{\sin^2(\epsilon)}\right)^2 \exp\left\{-v_{\text{sat}}^2\tau^2/(h^2 \varphi_{\text{RX}}^2) \log(2)\right\}, \tag{2}
\end{displaymath}
 -->
</P>
<DIV CLASS="displaymath">
<IMG
 STYLE="height: 6.40ex; vertical-align: -2.47ex; " SRC="img20.svg"
 ALT="$\displaystyle K_{P}(\tau)=k \exp\left\{-D\tau^2 \log(2)\right\}= \frac{p_t^2}{2...
...t\{-v_{\text{sat}}^2\tau^2/(h^2 \varphi_{\text{RX}}^2) \log(2)\right\}, \tag{2}$">
</DIV><P></P>
where <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.29ex; vertical-align: -0.57ex; " SRC="img21.svg"
 ALT="$k,D&gt;0$"></SPAN> are constant parameters that depend on the <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.92ex; vertical-align: -0.31ex; " SRC="img7.svg"
 ALT="$-3$"></SPAN> dB width of the antenna gain <!-- MATH
 $\varphi_{\text{RX}}$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 1.68ex; vertical-align: -0.57ex; " SRC="img22.svg"
 ALT="$\varphi_{\text{RX}}$"></SPAN>, interference source power <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.68ex; vertical-align: -0.57ex; " SRC="img23.svg"
 ALT="$p_t$"></SPAN>, the elevation angle <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.23ex; vertical-align: -0.12ex; " SRC="img24.svg"
 ALT="$\epsilon$"></SPAN>, the altitude of the LEO satellite <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.84ex; vertical-align: -0.12ex; " SRC="img25.svg"
 ALT="$h$"></SPAN>, the orbital speed <!-- MATH
 $v_{\text{sat}}$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 1.58ex; vertical-align: -0.46ex; " SRC="img26.svg"
 ALT="$v_{\text{sat}}$"></SPAN>, and the density of the Earth signal sources <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.84ex; vertical-align: -0.12ex; " SRC="img27.svg"
 ALT="$\lambda$"></SPAN>.

<P>
In our settings, since the antenna is directed to the Earth center, <!-- MATH
 $\epsilon = 90^{\circ} = \pi/2$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img28.svg"
 ALT="$\epsilon = 90^{\circ} = \pi/2$"></SPAN> rad. Further, for a normalized <!-- MATH
 $\mathbb{E}(P)$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img29.svg"
 ALT="$\mathbb{E}(P)$"></SPAN> (<SPAN CLASS="MATH"><IMG
 STYLE="height: 1.68ex; vertical-align: -0.57ex; " SRC="img23.svg"
 ALT="$p_t$"></SPAN> can be scaled accordingly) and for our satellite settings 2-3, <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.84ex; vertical-align: -0.12ex; " SRC="img30.svg"
 ALT="$k=0.5$"></SPAN> and <!-- MATH
 $D\approx 0.1352$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 1.81ex; vertical-align: -0.12ex; " SRC="img31.svg"
 ALT="$D\approx 0.1352$"></SPAN>. 
                 
</DD>
</DL>
                   Note that, given an antenna pattern width, the constant <!-- MATH
 $D=v_{\text{sat}}^2\tau^2/(h^2 \varphi_{\text{RX}}^2)$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.75ex; vertical-align: -0.76ex; " SRC="img32.svg"
 ALT="$D=v_{\text{sat}}^2\tau^2/(h^2 \varphi_{\text{RX}}^2)$"></SPAN> in (2) is directly defined by the altitude <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.84ex; vertical-align: -0.12ex; " SRC="img25.svg"
 ALT="$h$"></SPAN> of the satellite: the <A ID="tex2html37"
  HREF="https://en.wikipedia.org/wiki/Orbital_speed">orbital speed</A>
<!-- MATH
 $v_{\text{sat}}$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 1.58ex; vertical-align: -0.46ex; " SRC="img26.svg"
 ALT="$v_{\text{sat}}$"></SPAN> follows single-handedly from the orbital altitude. It can be empirically verified that within a fairly general density region, the assertion of <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.81ex; vertical-align: -0.12ex; " SRC="img33.svg"
 ALT="$D$"></SPAN> is more crucial for the GPR estimation than <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.84ex; vertical-align: -0.12ex; " SRC="img34.svg"
 ALT="$k$"></SPAN> (especially in the non-causal prediction): too large and too small <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.81ex; vertical-align: -0.12ex; " SRC="img33.svg"
 ALT="$D$"></SPAN> yield <A ID="tex2html38"
  HREF="https://en.wikipedia.org/wiki/Overfitting">overfitting</A>
and underfitting the data, respectively. In a refined version of the GPR, <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.84ex; vertical-align: -0.12ex; " SRC="img34.svg"
 ALT="$k$"></SPAN> and <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.81ex; vertical-align: -0.12ex; " SRC="img33.svg"
 ALT="$D$"></SPAN> could also be treated as <A ID="tex2html39"
  HREF="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">hyperparameters</A>, which are learned from the data, but we treat them as fixed.

<P>
The prior model we will utilize in the GPR is the properties 5-7. The described signal power <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img18.svg"
 ALT="$P(t)$"></SPAN> with the Gaussian autocovariance is the hypothesis class of the set of functions within which we search our estimation from, and the waveform <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img35.svg"
 ALT="$I(t)$"></SPAN> is treated as a non-stationary <A ID="tex2html40"
  HREF="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian process</A>. In practice, we will use the samples from <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.69ex; vertical-align: -0.70ex; " SRC="img36.svg"
 ALT="$\vert I(t)\vert^2$"></SPAN> to estimate the interference power <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img18.svg"
 ALT="$P(t)$"></SPAN>. Each such sample <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.58ex; vertical-align: -0.46ex; " SRC="img37.svg"
 ALT="$z_i$"></SPAN> is interpreted as noisy measurements of <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img18.svg"
 ALT="$P(t)$"></SPAN>: <!-- MATH
 $z_i= P(t_i) + \mathcal{N}(0,\sigma^2)$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.69ex; vertical-align: -0.70ex; " SRC="img38.svg"
 ALT="$z_i= P(t_i) + \mathcal{N}(0,\sigma^2)$"></SPAN>, where <!-- MATH
 $\sigma^2=3$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.12ex; vertical-align: -0.12ex; " SRC="img39.svg"
 ALT="$\sigma^2=3$"></SPAN> is set to correspond to the expected variance of <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.69ex; vertical-align: -0.70ex; " SRC="img36.svg"
 ALT="$\vert I(t)\vert^2$"></SPAN>. 

<P>

<DIV class="CENTER"><A ID="167"></A>
<TABLE>
<CAPTION class="BOTTOM"><STRONG>Figure 3:</STRONG>
Signal power estimated from scarce signal samples.</CAPTION>
<TR><TD><P>
<IMG
 STYLE="height: 46.69ex; vertical-align: -0.12ex; " SRC="img40.svg"
 ALT="\includegraphics[width=0.97\linewidth]{GPRvsMA.pdf}"></TD></TR>
</TABLE>
</DIV>

<P>
For a random realization of the power <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img18.svg"
 ALT="$P(t)$"></SPAN> (generated according to the gamma distribution and its second-order statistics), which is &ldquo;not known&rdquo; prior to the empirical samples, in Figures 1 (a) and 1 (b), we estimate <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img18.svg"
 ALT="$P(t)$"></SPAN> from a sampled (<SPAN CLASS="MATH"><IMG
 STYLE="height: 1.72ex; vertical-align: -0.12ex; " SRC="img41.svg"
 ALT="$0.1$"></SPAN> kHz) realization of the interference waveform <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img35.svg"
 ALT="$I(t)$"></SPAN> by using GPR and non-<A ID="tex2html41"
  HREF="https://en.wikipedia.org/wiki/Causal_filter">causal</A>
MA. The window size of the MA is empirically optimized, but it could also be learned from training data. Apart from the slight overfitting, the MA captures <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img18.svg"
 ALT="$P(t)$"></SPAN> reasonably well (as long as the window size is optimized). However, the GPR prediction is clearly better in capturing the smoothness of <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img18.svg"
 ALT="$P(t)$"></SPAN>. Furthermore, the GPR prediction captures the <SPAN CLASS="MATH"><IMG
 STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img18.svg"
 ALT="$P(t)$"></SPAN> from <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.72ex; vertical-align: -0.12ex; " SRC="img42.svg"
 ALT="$t=0$"></SPAN> onward, which is impossible for the non-causal, centralized MA, which starts at <!-- MATH
 $t\approx 1.5$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 1.72ex; vertical-align: -0.12ex; " SRC="img43.svg"
 ALT="$t\approx 1.5$"></SPAN>s. The <A ID="tex2html42"
  HREF="https://en.wikipedia.org/wiki/Predictive_analytics">forecast</A>
region in the figure corresponds to a causal estimation of the future signal based on the past values. Not so surprisingly, the GPR is superior to MA in this regard. Both predictions can be improved by increasing the sampling frequency.

<P>
Figure 1 (a) speaks its own language for the benefit of the prior SG-based hypothesis class versus the simple MA. On the contrary, without any hypothesis classes at hand, signal estimation would be much more difficult and arbitrary: even if the Gaussian correlation function is a commonly used initial guess for the autocovariance, and could be guessed without the SG analysis, the insight into the hyperparameters and their dependency on the orbital and network properties is invaluable. 

<P>
This was a simple, however theoretically solidly grounded, example of how SG analysis of wireless networks can assist in producing prior knowledge for the ML algorithms. A similar inquiry extends to signals affected by fading and other signal attenuation in LEO networks. Of course, terrestrial network settings are also feasible as hypothesis classes (having their distinct characteristics). The potential applications include <A ID="tex2html43"
  HREF="https://en.wikipedia.org/wiki/Self-interference_cancellation">interference cancellation</A>, <A ID="tex2html44"
  HREF="https://en.wikipedia.org/wiki/Error_correction_code">error correction</A>, and <A ID="tex2html45"
  HREF="https://en.wikipedia.org/wiki/Medium_access_control">medium access control</A>.

<P>
The following Python code generates the plots. (Written with the help of ChatGPT.)
<PRE>
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, gamma
from scipy.linalg import cholesky
from numpy.linalg import solve

# === Parameters ===
T_total = 15.0       # simulate P(t), I(t) on [0, 15] s
T_obs = 10.0         # we only "observe" up to 10 s
dt = 0.001           # fine time step for continuous path (1000 Hz)
t = np.arange(0, T_total, dt)
N = len(t)

fs_sample = 100.0     # sampling of |I|^2 (0.1 kHz)
sample_interval = 1.0 / fs_sample  # 0.1 s
# indices of sampled points up to T_obs
indices = np.arange(0, int(T_obs / dt), int(sample_interval / dt))
t_sampled = t[indices]

# Gamma marginal for P(t): mean=1, var=1/2 -&gt; Gamma(k=2, theta=0.5)
k_shape = 2.0
theta_scale = 0.5
mean_P = 1.0

# === 1. Construct latent Gaussian process for P(t) with covariance C(τ) = 0.5 * exp(-3 τ^2) ===
tau_full = t[:, None] - t[None, :]
k_emp = 0.5
D_emp = 0.1352
C_full = k_emp * np.exp(-D_emp * tau_full**2)

# numerical jitter for Cholesky
jitter = 1e-10
C_full += jitter * np.eye(N)

L = cholesky(C_full, lower=True)

# latent Gaussian process
z_latent = L @ np.random.randn(N)

# Gaussian copula to get Gamma process P(t)
u = norm.cdf(z_latent)
P = gamma.ppf(u, a=k_shape, scale=theta_scale)

# === 2. Generate I(t) = X(t)*sqrt(P(t)) ===
X = np.random.randn(N)
I = X * np.sqrt(P)

# === 3. Sample I and form |I|^2 on [0, T_obs] at 0.1 kHz ===
I_sampled = I[indices]
power_sampled = np.abs(I_sampled)**2   # z_i = |I(t_i)|^2

# === 4. 300-sample moving average on sampled powers (0..T_obs) ===
window_size = 300
kernel = np.ones(window_size) / window_size
power_ma = np.convolve(power_sampled, kernel, mode='valid')

# time axis for centered MA
if window_size &gt; 1:
    t_ma = t_sampled[(window_size-1)//2 : -(window_size//2)]
else:
    t_ma = t_sampled

# Simple MA "forecast": extend last MA value flat from last t_ma to T_total
t_ma_forecast = np.arange(t_ma[-1], T_total + 1e-9, sample_interval)  # 0.1 kHz grid to end
power_ma_forecast = np.full_like(t_ma_forecast, power_ma[-1])

# Approximate noise variance for |I|^2
# Var(|I|^2 | P=p) = 2 p^2, E[P^2]=1.5 =&gt; average Var ≈ 3
noise_var = 3.0

# Variance of 300-sample MA under iid noise with var=noise_var
ma_var = noise_var / window_size
ma_std = np.sqrt(ma_var)

# === 5. GPR prediction of P(t) on [0, 15] from noisy power samples on [0, 10] ===
# GP prior: P ~ GP(mean=mean_P, cov(t,s) = 0.5*exp(-3(t-s)^2))
# Observations: z_i = |I(t_i)|^2 ≈ P(t_i) + N(0, noise_var)

# training times (sampled up to T_obs)
t_train = t_sampled
z_train = power_sampled

# Covariance among training points
tau_ss = t_train[:, None] - t_train[None, :]
k_prior = 0.5
D_prior = 0.1352
C_ss = k_prior * np.exp(-D_prior * tau_ss**2 )
# Covariance between all times (0..T_total) and training points
tau_fs = t[:, None] - t_train[None, :]
C_fs = k_prior * np.exp(-D_prior * tau_fs**2 )


# Add observation noise to training covariance
C_ss_noisy = C_ss + noise_var * np.eye(len(t_train))

# Posterior mean: m_post(t) = mean_P + C_fs * K^{-1} * (z - mean_P)
z_centered = z_train - mean_P
alpha = solve(C_ss_noisy, z_centered)
P_gp_mean = mean_P + C_fs @ alpha

# Posterior variance: diag(C_ff - C_fs K^{-1} C_sf)
C_ff_diag = np.full(N, 0.5)  # since C_ff(t,t) = 0.5 for all t
V = solve(C_ss_noisy, C_fs.T)  # shape: (n_train, N)
C_fs_V = np.sum(C_fs * V.T, axis=1)
P_gp_var = C_ff_diag - C_fs_V
P_gp_std = np.sqrt(np.maximum(P_gp_var, 0.0))

# === 6. Plot ===
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), sharex=True)

# ---------- Subplot (a): P(t), GPR forecast, MA forecast ----------
# True P(t) over [0, 15]
ax1.plot(t, P, label='True signal power P(t)', color='C0', alpha=0.7)

# GPR mean and ±2σ over [0, 15]
ax1.plot(t, P_gp_mean, label='GPR prediction of P(t)', color='C1', linewidth=2)
# ax1.fill_between(t,
#                  P_gp_mean - 2*P_gp_std,
#                  P_gp_mean + 2*P_gp_std,
#                  color='C1', alpha=0.2, label='GPR ±2σ')

# 300-sample MA on [0, T_obs]
ax1.plot(t_ma, power_ma, 'r-', label='MA of |I|² (0–10 s)')
# ax1.fill_between(t_ma,
#                  power_ma - 2*ma_std,
#                  power_ma + 2*ma_std,
#                  color='r', alpha=0.15, label='MA ±2σ (approx, 0–10 s)')

# Flat MA forecast beyond last MA point
ax1.plot(t_ma_forecast, power_ma_forecast, 'r--', label='MA forecast (flat)')

# Raw power samples (0–10 s)
# ax1.plot(t_sampled, power_sampled, 'kx', label='Samples |I(t)|² (0.1 kHz)', markersize=6)

# Mark observation horizon
ax1.axvline(T_obs, color='k', linestyle=':', alpha=0.5)
ax1.text(T_obs+0.05, ax1.get_ylim()[1]*0.9, 'Forecast region', fontsize=12)

ax1.set_ylabel('Signal power', fontsize=12)
ax1.set_title('Fig 1(a): True P(t), GPR estimate &amp; forecast, and 300-sample MA &amp; forecast', fontsize=12)
ax1.grid(True)
ax1.legend(fontsize=12)

# ---------- Subplot (b): I(t) and samples ----------
ax2.plot(t, I, label='I(t) = X(t)√P(t)', color='tab:orange')
ax2.plot(t_sampled, I_sampled, 'ko', label='Samples of I(t) (0–10 s, 0.1 kHz)', markersize=1)
ax2.axvline(T_obs, color='k', linestyle=':', alpha=0.5)
ax2.set_xlabel('t [s]')
ax2.set_ylabel('Amplitude', fontsize=12)
ax2.set_title('Fig 1(b): Non-stationary Gaussian noise I(t) with sampled observations', fontsize=12)
ax2.grid(True)
ax2.legend(fontsize=12)

plt.tight_layout()
plt.show()
</PRE>

<P>
References:
<table width="90%"><tr><td align="right" valign="top">[1]</td><td valign="top">&nbsp;Y. Hmamouche, M. Benjillali, S. Saoudi, H. Yanikomeroglu and M. D. Renzo, "New Trends in Stochastic Geometry for Wireless Networks: A Tutorial and Survey," in Proceedings of the IEEE, vol. 109, no. 7, pp. 1200-1252, July 2021, doi: 10.1109/JPROC.2021.3061778 </td></tr>
<tr><td align="right" valign="top">[2]</td><td valign="top">&nbsp;<A ID="tex2html46"
  HREF="dissertation.pdf">Angervuori, Ilari., Narrow-Beam Low Earth Orbit Communications and Stochastic Geometry: Non-Temporal and Temporal Interference Analysis, Thesis Draft; Aalto University</A></td></tr>
<tr><td align="right" valign="top">[3]</td><td valign="top">&nbsp;<A ID="tex2html47"
  HREF="https://aaltodoc.aalto.fi/server/api/core/bitstreams/c9162271-264d-46f8-8796-efb4f8a4724e/content">Oksanen, Maiju., Stokastinen geometria langattoman

tietoliikenteen analyysissä, opinnäytetyö; aalto-yliopisto.</A>
<P>
</td></tr></table>

<P>

<DIV CLASS="navigation"><HR>
<!--Navigation Panel-->
<A
 HREF="node10.html">
<IMG WIDTH="37" HEIGHT="24" ALT="next" SRC="next.png"></A> 
<A
 HREF="node8.html">
<IMG WIDTH="26" HEIGHT="24" ALT="up" SRC="up.png"></A> 
<A
 HREF="node8.html">
<IMG WIDTH="63" HEIGHT="24" ALT="previous" SRC="prev.png"></A> 
<A ID="tex2html228"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALT="contents" SRC="contents.png"></A>  <A ID="tex2html2"
  HREF="cv.pdf"><IMG
 STYLE="" SRC="jetscalecropped.png"
 ALT="8"></A>
<BR>
<B> Next:</B> <A
 HREF="node10.html">Blog posts 2023</A>
<B> Up:</B> <A
 HREF="node8.html">Blog posts 2026</A>
<B> Previous:</B> <A
 HREF="node8.html">Blog posts 2026</A>
 &nbsp; <B>  <A ID="tex2html229"
  HREF="node1.html">Contents</A></B> </DIV>
<!--End of Navigation Panel-->

</BODY>
</HTML>
